# Optimization-Gradient-Descent-with-Linear-Regression
From scratch, we are implementing gradient descent with linear regression using Adagrad, RMSProp and Ada. then comparing them.
